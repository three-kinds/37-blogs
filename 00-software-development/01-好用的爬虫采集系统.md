# 好用的爬虫采集系统

## 0. 背景

* 我有过好几年的爬虫开发经历，接口爬虫、自动化爬虫都做过一些，深刻了解采集系统的重要性
* 我写过几年的后端应用，认为后端有很多好用的经验与轮子
* 我写过几版采集系统，期望通过文本记录自己追求好用的采集系统的历程

## 1. 采集系统是怎样发展而来的

1. 爬虫的开发与执行有较相似的流程，如获取数据、提取数据、保存数据；在这一步，有很多优秀的开源框架，如[Scrapy](https://github.com/scrapy/scrapy)，让我们专注于爬虫特有逻辑开发
2. 需要更高效采集时，通常会通过集群去增大爬虫规模；在这一步，有[Scrapy-Redis](https://github.com/rmax/scrapy-redis)帮助Scrapy爬虫集群化
3. 为了保证稳定的数据供给，通常需要任务管理、运行监控、告警等运维能力；在这一步，有[ScrapydWeb](https://github.com/my8100/scrapydweb)帮助我们管理Scrapy集群
4. 它们共同组成了采集系统：将共有的逻辑从具体爬虫提取到通用的组件、框架、系统中，让开发者专注于爬虫本身的开发

## 2. 我心目中好用的采集系统

1. 开发舒适：通用的逻辑封装的足够好，开发者可专注于爬虫站点流程与难点；不用担心框架限制，各种类型的爬虫都支持、各种组件都可选、甚至不挑编程语言
2. 便于测试：应当支持并易于单元测试、整体流程测试（像普通的后端应用那样）
3. 运行模式的切换：应当在不改代码的情况下就可以切换运行模式，单机运行、集群运行（像普通的后端应用切换环境那样）
4. 任务管理：任务调度与管理由采集系统或外部系统维护，爬虫不应当也不知道自己目前在运行的是什么业务或者调度优先级
5. 通用工具：通用工具如告警、代理应当服务化，便于这些通用服务独立升级与规模缩放
6. 运维：普通的监控、日志、规模缩放应当要复用现成的工具，如Prometheus、ELK、Kubernetes等

### 2.1 开发舒适

1. 专注爬虫开发：通用功能都有现成的组件，这需要一个好用且没有多少约束的爬虫开发框架
2. 语言自由选择：这需要每个语言都有这样的一个框架
3. 组件自由选择：框架没有过多的限制和绑架、这样集成其它好用的组件就比较容易，甚至框架的所有组件都是可替换的
4. 各种爬虫类型都支持：支持接口采集、桌面程序自动化、Web自动化、App自动化、自动化+hook/抓包等类型
5. 本质上就是框架限制少、支持的组件才多、通用的功能才丰富、开发者才可以专注

### 2.2 便于测试

1. 单元测试与覆盖测试是很重要的，一定要支持；解析流程有充分的单元测试、覆盖测试，可以明显减轻心智负担
2. 整体流程的本地可测试便于本地复现问题，也能在CI上自动测试

### 2.3 运行模式的切换

1. 应当在不改代码的情况下就可以切换运行模式，如：单机模式、集群节点模式
2. 后端应用通常可以通过不同的配置来改变不同环境依赖的组件与具体配置，这值得借鉴

### 2.4 任务管理

1. 爬虫只专注于单元采集任务的处理，只处理单元采集流程，爬虫不应当也不知道自己目前在运行的是什么业务或者调度优先级
2. 即使是性能要求非常高的实时响应任务、或量非常大的批量采集任务，它们也都是业务任务
3. 无论是业务相关的任务还是集群相关的任务，都不应当耦合到爬虫中、应当由外部系统、采集系统来维护；否则爬虫的复杂度将不可控

### 2.5 通用工具

1. 像告警、代理、验证码识别等通用工具，应当服务化，通过不变的接口去使用，一方面可以让爬虫更专注，另一方面当工具服务核心更换时，爬虫也无需改动
2. 对服务来说，它们可以独立升级与按需规模缩放
3. 一些通具工具作为本地组件合适、一些服务化更合适，可以结合资源消耗程度、资源共享需求来做选择

### 2.6 运维

1. 运维工具的选择，要优先考虑是否可以接入现有的运维系统，比如运维常用的Prometheus、ELK、Kubernetes，好用且接入简单
2. 爬虫节点规模的缩放、通用服务的规模缩放等运维方法，都应当考虑使用现有的工具，而不是自己造轮子
3. 轮子的选择，首要考虑社区成熟度，上手难度肯定不是首要选择、很多上手难度低的工具上限较低，被广泛使用的轮子上手难度通常也不会太高
4. 只有像站点改版监控这样关联面可能较多的需求，才考虑自己去实现：通过结合采集系统的任务管理与确定的目标数据结果来实现站点改版的监控
